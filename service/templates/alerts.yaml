{{- if not .Values.schedule -}}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  generation: 1
  labels:
    app: kube-prometheus-stack
    heritage: Helm
    release: prometheus
  name: {{ .Values.name }}
spec:
  groups:
    - name: {{ .Values.name }}.rules
      rules:
        # Alert if the number of pods goes down minimum over a period of time.
        - alert: {{ snakecase .Values.name }}_pod_below_minimum_replicas
          annotations:
            description: "{{ .Values.name }} pods below minimum number of replicas."
          expr: sum(kube_deployment_status_replicas_available{namespace="{{ .Release.Namespace }}", deployment="{{ .Values.name }}"}) - sum(kube_horizontalpodautoscaler_spec_min_replicas{namespace="{{ .Release.Namespace }}", horizontalpodautoscaler="{{ .Values.name }}"}) < 0
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if the pod restarts.
        - alert: {{ snakecase .Values.name }}_pod_restarts
          annotations:
            description: "{{ .Values.name }} pods restarts"
          expr: sum(increase(kube_pod_container_status_restarts_total{namespace="{{ .Release.Namespace }}",pod=~"{{ .Values.name }}-.*" }[{{ .Values.alerts.standard.infra.pod_restart_time_window }}])) > {{ .Values.alerts.standard.infra.pod_restart_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if replicas of service are in unavailable state.
        - alert: {{ snakecase .Values.name }}_unavailable_replicas
          annotations:
            description: "{{ .Values.name }} required number of replicas are unavailable"
          expr: sum(kube_deployment_status_replicas_unavailable{deployment="{{ .Values.name }}",namespace="{{ .Release.Namespace }}"}) > {{ .Values.alerts.standard.infra.unavailable_replicas_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if the deployment has zero replicas.
        - alert: {{ snakecase .Values.name }}_deployment_has_zero_replicas
          annotations:
            description: "{{ .Values.name }} deployment has zero replicas."
          expr: sum(kube_deployment_status_replicas{namespace="{{ .Release.Namespace }}",  deployment="{{ .Values.name }}"}) == 0
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        #  Alert if replica count crosses the threshold percentage of max pod count.
        - alert: {{ snakecase .Values.name }}_hpa_nearing_max_pod_count
          annotations:
            description: "{{ .Values.name }} nearing HPA max pod count threshold."
          expr: (sum(kube_horizontalpodautoscaler_status_current_replicas{namespace="{{ .Release.Namespace }}", horizontalpodautoscaler="{{ .Values.name }}"}) / sum(kube_horizontalpodautoscaler_spec_max_replicas{namespace="{{ .Release.Namespace }}", horizontalpodautoscaler="{{ .Values.name }}"})) * 100 >= {{ .Values.alerts.standard.infra.hpa_nearing_max_pod_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if memory utilization is beyond threshold for service in any pod
        - alert: {{ snakecase .Values.name }}_service_memory_utilization_above_{{ .Values.alerts.standard.infra.service_memory_utilization_threshold}}
          annotations:
            description: "{{ .Values.name }}  service memory utilization above {{ .Values.alerts.standard.infra.service_memory_utilization_threshold}} percentage"
          expr: sum(container_memory_working_set_bytes{pod=~"{{ .Values.name }}-.*", container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}) / sum(container_spec_memory_limit_bytes{pod=~"{{ .Values.name }}-.*",container="{{ .Values.name }}",namespace="{{ .Release.Namespace }}"}) *100 > {{ .Values.alerts.standard.infra.service_memory_utilization_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if CPU utilization is beyond threshold for service in any pod
        - alert: {{ snakecase .Values.name }}_service_cpu__utilization_above_{{ .Values.alerts.standard.infra.service_cpu_utilization_threshold}}
          annotations:
            description: "{{ .Values.name }} service cpu utilization above {{ .Values.alerts.standard.infra.service_cpu_utilization_threshold}} percentage"
          expr: ((sum(irate(container_cpu_usage_seconds_total{container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"}[{{ .Values.alerts.standard.infra.service_cpu_utilization_time_window }}]))  / sum(container_spec_cpu_quota{container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"} / container_spec_cpu_period{container="{{ .Values.name }}", namespace="{{ .Release.Namespace }}"})  ) * 100)  > {{ .Values.alerts.standard.infra.service_cpu_utilization_threshold }} 
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        # Alert if  application health-check failure increased
        {{- if ne .Values.alerts.standard.infra.health_check_failure_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_healthcheck_failure_increased
          annotations:
            description: "{{ .Values.name }} apps health-check failure beyond threshold."
          expr: sum(increase(zs_http_response_count{namespace="{{ .Release.Namespace }}",path="/.well-known/health-check",status!="200",service="{{ .Values.name }}"}[{{ .Values.alerts.standard.infra.health_check_failure_time_window }}]))> {{ .Values.alerts.standard.infra.health_check_failure_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
         {{- end }}

        {{- range $k, $v := .Values.alerts.standard.response_code}}
        # Alert if the any response code errors in application reaches a threshold.
        {{- if ne $v.absolute_threshold -1.0 }}
        - alert: {{ snakecase $.Values.name }}_{{ snakecase $k }}_error
          annotations:
            description: "{{ $.Values.name }} apps throwing {{ $k }} error beyond threshold."
          {{- if eq $k "5xx"}}
          expr: sum(increase(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}]))> {{ $v.absolute_threshold }}
          {{- else if eq $k "201"}}
          expr: sum(increase(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}])) < {{ $v.absolute_threshold }}
          {{- else}}
          expr: sum(increase(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}]))> {{ $v.absolute_threshold }}
          {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        # Alert if the any response code errors in application reaches greater than percentage threshold.
        {{- if ne $v.percentage_threshold -1.0}}
        - alert: {{ snakecase $.Values.name }}_{{ snakecase $k }}_error_percentage
          annotations:
            description: "{{ $.Values.name }} apps throwing {{ $k }} error beyond percentage threshold."
          {{- if eq $k "5xx"}}
          expr: sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 > {{ $v.percentage_threshold }}
          {{- else if eq $k "201"}}
          expr: sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 < {{ $v.percentage_threshold }}
          {{- else }}
          expr: sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 > {{ $v.percentage_threshold }}
          {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        # Alert if the any response code count is 15% (configurable) in last 5 mins (configurable) greater than 5 min (configurable) for last 24h (configurable).
        {{- if ne $v.adaptive_threshold -1.0}}
        - alert: {{ snakecase $.Values.name }}_{{snakecase $k }}_error_adaptive_alerts
          annotations:
           description: "{{ $.Values.name }} throwing {{ $k }} errors {{ $v.adaptive_threshold }}% in last {{ $v.adaptive_time_window }} over previous {{ $v.adaptive_reference_time_window }}"
          {{- if eq $k "5xx"}}
          expr: ((sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 > {{ $v.adaptive_threshold }}
          {{- else if eq $k "201"}}
          expr: ((sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 < {{ $v.adaptive_threshold }}
          {{- else }}
          expr: ((sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 > {{ $v.adaptive_threshold }}
          {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        {{- end }}

        {{- range $k, $v := .Values.alerts.standard.outbound_response_code }}
        {{- if ne $v.absolute_threshold -1.0 }}
        # Alert if the any outbound response code errors in application reaches a threshold.
        - alert: {{ snakecase $.Values.name }}_{{ snakecase $k }}_outbound_error
          annotations:
            description: "{{ $.Values.name }} apps throwing {{ $k }} outbound error beyond threshold."
          {{- if eq $k "5xx"}}
          expr: sum(increase(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}]))> {{ $v.absolute_threshold }}
          {{- else if eq $k "201"}}
          expr: sum(increase(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}])) < {{ $v.absolute_threshold }}
          {{- else}}
          expr: sum(increase(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.absolute_time_window }}]))> {{ $v.absolute_threshold }}
          {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        # Alert if the any outbound response code errors in reaches greater than percentage threshold.
        {{- if ne $v.percentage_threshold -1.0 }}
        - alert: {{ snakecase $.Values.name }}_{{ snakecase $k }}_outbound_error_percentage
          annotations:
            description: "{{ $.Values.name }} apps throwing {{ $k }} outbound error beyond percentage threshold."
          {{- if eq $k "5xx"}}
          expr: sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 > {{ $v.percentage_threshold }}
          {{- else if eq $k "201"}}
          expr: sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 < {{ $v.percentage_threshold }}
          {{- else}}
          expr: sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",service="{{ $.Values.name }}" }[{{ $v.percentage_time_window }}]))* 100 > {{ $v.percentage_threshold }}
          {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        # Alert if the any outbound response code count is 15% (configurable) in last 5 mins (configurable) greater than 5 min (configurable) for last 24h (configurable).
        {{- if ne $v.adaptive_threshold -1.0}}
        - alert: {{ snakecase $.Values.name }}_{{snakecase $k }}_error_adaptive_alerts
          annotations:
            description: "{{ $.Values.name }} throwing {{ $k }} outbound errors {{ $v.adaptive_threshold }}% in last {{ $v.adaptive_time_window }} over previous {{ $v.adaptive_reference_time_window }}"
          {{- if eq $k "5xx"}}
          expr: ((sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status=~"5..",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 > {{ $v.adaptive_threshold }}
          {{- else if eq $k "201"}}
          expr: ((sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 < {{ $v.adaptive_threshold }}
          {{- else }}
          expr: ((sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_time_window }}]))/sum(rate(zs_http_service_response_count{ namespace="{{ $.Release.Namespace }}",status="{{ $k }}",service="{{ $.Values.name }}"}[{{ $v.adaptive_reference_time_window }}]))) * 100) - 100 > {{ $v.adaptive_threshold }}
            {{- end }}
          labels:
            severity: {{ $v.severity_level | default "warning"}}
            servicealert: "true"
            service: {{ $.Values.name }}
            namespace: {{ $.Release.Namespace }}
        {{- end }}
        {{- end }}

        # Alert if 95 percentile application response time increases
        {{- if ne .Values.alerts.standard.response_time.absolute_warning_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_95th_percentile_response_time_increases
          annotations:
            description: "{{ .Values.name }} apps response is more than threshold."
          expr: (histogram_quantile(0.95, sum(rate(zs_http_response_bucket{service="{{ .Values.name }}"}[{{ .Values.alerts.standard.response_time.absolute_warning_time_window }}])) by (le)))> {{ .Values.alerts.standard.response_time.absolute_warning_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Alert if 95 percentile application response time increases beyond critical
        {{- if ne .Values.alerts.standard.response_time.absolute_critical_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_95th_percentile_response_time_critical
          annotations:
            description: "{{ .Values.name }} apps response is more than critical threshold."
          expr: (histogram_quantile(0.95, sum(rate(zs_http_response_bucket{service="{{ .Values.name }}"}[{{ .Values.alerts.standard.response_time.absolute_critical_time_window }}])) by (le)))> {{ .Values.alerts.standard.response_time.absolute_critical_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Trigger alert if the 99%ile (configurable) application response time for last 5 mins (configurable) is more than 15% (user configurable) over last 1 day (user configurable)
        {{- if ne .Values.alerts.standard.response_time.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_response_time
          annotations:
            description: "{{ .Values.name }} {{ .Values.alerts.standard.response_time.adaptive_percentile}}ile {{.Values.alerts.standard.response_time.adaptive_threshold}}% higher for last {{.Values.alerts.standard.response_time.adaptive_time_window}} over previous {{.Values.alerts.standard.response_time.adaptive_reference_time_window}}"
          expr:  ((histogram_quantile({{.Values.alerts.standard.response_time.adaptive_percentile}}, sum(rate(zs_http_response_bucket{ namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.response_time.adaptive_time_window }}])) by (le)))/(histogram_quantile({{.Values.alerts.standard.response_time.adaptive_percentile }}, sum(rate(zs_http_response_bucket{ namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.response_time.adaptive_reference_time_window }}])) by (le)))* 100) - 100 > {{ .Values.alerts.standard.response_time.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}

        # Alert if 95 percentile application outbound response time increases
        {{- if ne .Values.alerts.standard.outbound_response_time.absolute_warning_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_95th_percentile_outbound_response_time_increases
          annotations:
            description: "{{ .Values.name }} apps response is more than threshold."
          expr: (histogram_quantile(0.95, sum(rate(zs_http_service_response_bucket{service="{{ .Values.name }}"}[{{ .Values.alerts.standard.outbound_response_time.absolute_warning_time_window }}])) by (le)))> {{ .Values.alerts.standard.outbound_response_time.absolute_warning_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Alert if 95 percentile application outbound response time increases beyond critical
        {{- if ne .Values.alerts.standard.outbound_response_time.absolute_critical_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_95th_percentile_outbound_response_time_critical
          annotations:
            description: "{{ .Values.name }} apps response is more than critical threshold."
          expr: (histogram_quantile(0.95, sum(rate(zs_http_service_response_bucket{service="{{ .Values.name }}"}[{{ .Values.alerts.standard.outbound_response_time.absolute_critical_time_window }}])) by (le)))> {{ .Values.alerts.standard.outbound_response_time.absolute_critical_threshold }}
          labels:
            severity: critical
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Trigger alert if the 99%ile (configurable) application outbound response time for last 5 mins (configurable) is more than 15% (user configurable) over last 1 day (user configurable)
        {{- if ne .Values.alerts.standard.outbound_response_time.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_outbound_response_time
          annotations:
            description: "{{ .Values.name }} {{ .Values.alerts.standard.outbound_response_time.adaptive_percentile}}ile {{.Values.alerts.standard.outbound_response_time.adaptive_threshold}}% higher for last {{.Values.alerts.standard.outbound_response_time.adaptive_time_window}} over previous {{.Values.alerts.standard.outbound_response_time.adaptive_reference_time_window}}"
          expr:  ((histogram_quantile({{.Values.alerts.standard.outbound_response_time.adaptive_percentile}}, sum(rate(zs_http_service_response_bucket{ namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_response_time.adaptive_time_window }}])) by (le)))/(histogram_quantile({{.Values.alerts.standard.outbound_response_time.adaptive_percentile }}, sum(rate(zs_http_service_response_bucket{ namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_response_time.adaptive_reference_time_window }}])) by (le)))* 100) - 100 > {{ .Values.alerts.standard.outbound_response_time.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}

        # Trigger alert if the number of requests in the last 5 mins is higher than 30% than the average for last 24 hours
        {{- if ne .Values.alerts.standard.request_count.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_request_count_higher_threshold
          annotations:
            description: "{{ .Values.name }} request count {{.Values.alerts.standard.request_count.adaptive_threshold }}% higher for last {{.Values.alerts.standard.request_count.adaptive_time_window}} over pervious {{.Values.alerts.standard.request_count.adaptive_reference_time_window}}"
          expr: ((avg(rate(zs_http_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.request_count.adaptive_time_window }}]))/avg(rate(zs_http_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.request_count.adaptive_reference_time_window }}]))) * 100) > 100 + {{ .Values.alerts.standard.request_count.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Trigger alert if the number of requests in the last 5 mins is lower than 30% than the average for last 24 hours
        {{- if ne .Values.alerts.standard.request_count.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_request_count_lower_threshold
          annotations:
            description: "{{ .Values.name }} request count {{.Values.alerts.standard.request_count.adaptive_threshold}}% lower for last {{.Values.alerts.standard.request_count.adaptive_time_window}} over pervious {{.Values.alerts.standard.request_count.adaptive_reference_time_window}}"
          expr: ((avg(rate(zs_http_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.request_count.adaptive_time_window }}]))/avg(rate(zs_http_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.request_count.adaptive_reference_time_window }}]))) * 100) < 100 - {{ .Values.alerts.standard.request_count.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Trigger alert if the number of outbound requests in the last 5 mins is higher than 30% than the average for last 24 hours
        {{- if ne .Values.alerts.standard.outbound_request_count.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_outbound_request_count_higher_threshold
          annotations:
            description: "apps throwing {{ .Values.name }} outbound request count {{.Values.alerts.standard.outbound_request_count.adaptive_threshold}}% higher for last {{.Values.alerts.standard.outbound_request_count.adaptive_time_window}} over pervious {{.Values.alerts.standard.outbound_request_count.adaptive_reference_time_window}}"
          expr: ((avg(rate(zs_http_service_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_request_count.adaptive_time_window }}]))/avg(rate(zs_http_service_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_request_count.adaptive_reference_time_window }}]))) * 100) > 100 + {{ .Values.alerts.standard.outbound_request_count.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}
        # Trigger alert if the number of outbound requests in the last 5 mins is lower than 30% than the average for last 24 hours
        {{- if ne .Values.alerts.standard.outbound_request_count.adaptive_threshold -1.0 }}
        - alert: {{ snakecase .Values.name }}_adaptive_alerts_application_outbound_request_count_lower_threshold
          annotations:
            description: "apps throwing {{ .Values.name }} outbound request count {{.Values.alerts.standard.outbound_request_count.adaptive_threshold}}% lower for last {{.Values.alerts.standard.outbound_request_count.adaptive_time_window}} over pervious {{.Values.alerts.standard.outbound_request_count.adaptive_reference_time_window}}"
          expr: ((avg(rate(zs_http_service_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_request_count.adaptive_time_window }}]))/avg(rate(zs_http_service_response_count{namespace="{{ .Release.Namespace }}",service="{{ .Values.name }}" }[{{ .Values.alerts.standard.outbound_request_count.adaptive_reference_time_window }}]))) * 100) < 100 - {{ .Values.alerts.standard.outbound_request_count.adaptive_threshold }}
          labels:
            severity: warning
            servicealert: "true"
            service: {{ .Values.name }}
            namespace: {{ .Release.Namespace }}
        {{- end }}

        # Custom Alerts dependent on services
        {{- range $v := .Values.alerts.custom }}
        - alert: {{ snakecase $v.name }}
          annotations:
            description: {{ $v.description | quote }}
          {{- if ne (default 0.0 $v.percentile) 0.0 }}
          expr: (histogram_quantile({{ $v.percentile }}, sum(rate({{ $v.alert_rule }}{service="{{ $.Values.name }}"}[{{ $v.time_window }}])) by ({{ $v.sum_by_label }}))){{ $v.query_operator | default (printf ">") }} {{ $v.threshold }}
          {{- else }}
            {{- if  ne $v.sum_by_label ""  }}
               {{- if  ne $v.label_value ""  }}
          expr: sum by ({{ $v.sum_by_label }}) (increase({{ $v.alert_rule }}{service="{{ $.Values.name }}", namespace="{{ $.Release.Namespace }}"{{printf ", %s=\"%s\"" $v.sum_by_label $v.label_value }}{{ printf "}["}}{{ printf $v.time_window}}{{ printf "]))"}}{{ $v.query_operator | default (printf " > ") }}{{ $v.threshold}}
               {{- else}}
          expr: sum by ({{ $v.sum_by_label }}) (increase({{ $v.alert_rule }}{service="{{ $.Values.name }}", namespace="{{ $.Release.Namespace }}"{{ printf "}["}}{{ printf $v.time_window}}{{ printf "]))"}}{{  $v.query_operator | default (printf ">") }}{{ $v.threshold}}
               {{- end }}
            {{- end }}
            {{- if  eq $v.sum_by_label ""  }}
          expr: sum(increase({{ $v.alert_rule }}{service="{{ $.Values.name }}", namespace="{{ $.Release.Namespace }}"{{ printf "}["}}{{ printf $v.time_window}}{{ printf "]))"}}{{ $v.query_operator | default (printf ">") }}{{ $v.threshold}}
            {{- end }}
          {{- end }}
          labels:
            servicealert: "true"
            namespace: {{ $.Release.Namespace }}
            service: {{ $.Values.name }}
            {{- range $key,$value := $v.labels }}
            {{ $key }}: {{ $value }}
            {{- end }}
        {{- end }}
---
{{- end -}}